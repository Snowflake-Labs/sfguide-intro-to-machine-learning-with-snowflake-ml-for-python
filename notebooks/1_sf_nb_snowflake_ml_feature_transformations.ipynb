{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e94e2688-b943-452e-b0d9-7204867ac918",
   "metadata": {
    "name": "md_env",
    "collapsed": false,
    "resultHeight": 157
   },
   "source": "# IMPORTANT \nMake sure you've imported the [environment.yml](https://github.com/Snowflake-Labs/sfguide-intro-to-machine-learning-with-snowflake-ml-for-python/blob/main/notebooks/environment.yml) file provided in the git repo on the left sidebar.\n\nThis will ensure if you have the right packages needed to run this Notebook."
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0f7cf92-65a9-48b2-9212-d8807ba454bc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_feature_transformations",
    "resultHeight": 227
   },
   "source": [
    "## 2. ML Feature Transformations\n",
    "\n",
    "- In this notebook, we will walk through a few transformations that are included in the Snowpark ML Preprocessing API. \n",
    "- We will also build a preprocessing pipeline to be used in the ML modeling notebook.\n",
    "\n",
    "***Note: All feature transformations using Snowpark ML are distributed operations in the same way that Snowpark DataFrame operations are.***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96992c21-d30a-400e-a034-0bfd96a1200f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_import_libs",
    "resultHeight": 46
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd506e2-20a7-43fd-9630-7fc0468c9652",
   "metadata": {
    "language": "python",
    "name": "import_libs",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# Snowpark for Python\n",
    "import snowflake.snowpark.functions as F\n",
    "from snowflake.snowpark.types import DecimalType\n",
    "\n",
    "# Snowpark ML\n",
    "import snowflake.ml.modeling.preprocessing as snowml\n",
    "from snowflake.ml.modeling.pipeline import Pipeline\n",
    "from snowflake.ml.modeling.metrics.correlation import correlation\n",
    "\n",
    "# Data Science Libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Misc\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "# warning suppresion\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69454482-73d4-4d81-90ad-374152e5bcb9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_active_session",
    "resultHeight": 113
   },
   "source": [
    "### Establish Secure Connection to Snowflake\n",
    "\n",
    "Notebooks establish a Snowpark Session when the notebook is attached to the kernel. Let's use that Session object to validate our connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec397de-9822-4e60-aabe-969a630849f5",
   "metadata": {
    "language": "sql",
    "name": "init_sql",
    "resultHeight": 112
   },
   "outputs": [],
   "source": [
    "-- Using Warehouse, Database, and Schema created during Setup\n",
    "USE WAREHOUSE ML_HOL_WH;\n",
    "USE DATABASE ML_HOL_DB;\n",
    "USE SCHEMA ML_HOL_SCHEMA;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aeaefb-b580-47d3-8382-e915e33b1d4c",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "get_active_session",
    "resultHeight": 84
   },
   "outputs": [],
   "source": [
    "session = get_active_session()\n",
    "\n",
    "# Add a query tag to the session.\n",
    "session.query_tag = {\"origin\":\"sf_sit-is\", \n",
    "                     \"name\":\"e2e_ml_snowparkpython\", \n",
    "                     \"version\":{\"major\":1, \"minor\":0,},\n",
    "                     \"attributes\":{\"is_quickstart\":1}}\n",
    "session"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71ac9d3f-0e4e-466c-b500-868ab63b69e9",
   "metadata": {
    "name": "md_data_loading",
    "resultHeight": 46
   },
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aa8b77-5ab8-4953-b7b9-2c023a1d03cc",
   "metadata": {
    "language": "python",
    "name": "load_diamonds",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# First, we read in the data from a Snowflake table into a Snowpark DataFrame\n",
    "# **Change this only if you named your table something else in the data ingest notebook **\n",
    "diamonds_df = session.table(\"DIAMONDS\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a052fa9-922e-4d72-a41d-f1795bfac6ea",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_feature_transformation",
    "resultHeight": 113
   },
   "source": [
    "### Feature Transformations\n",
    "\n",
    "We will illustrate a few of the transformation functions here, but the rest can be found in the [documentation](https://docs.snowflake.com/LIMITEDACCESS/snowflake-ml-preprocessing)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "265507af-8e9c-4d8a-987d-914d249309c2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_scaling",
    "resultHeight": 31
   },
   "source": [
    "##### Let's use the `MinMaxScaler` to normalize the `CARAT` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c962dc-1724-4eec-a3b6-bd255e9df4bf",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "min_max_scaler",
    "resultHeight": 439
   },
   "outputs": [],
   "source": [
    "# Normalize the CARAT column\n",
    "snowml_mms = snowml.MinMaxScaler(input_cols=[\"CARAT\"], output_cols=[\"CARAT_NORM\"])\n",
    "normalized_diamonds_df = snowml_mms.fit(diamonds_df).transform(diamonds_df)\n",
    "\n",
    "# Reduce the number of decimals\n",
    "new_col = normalized_diamonds_df.col(\"CARAT_NORM\").cast(DecimalType(7, 6))\n",
    "normalized_diamonds_df = normalized_diamonds_df.with_column(\"CARAT_NORM\", new_col)\n",
    "\n",
    "normalized_diamonds_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d5292e4-613f-4f4c-9579-aa338cf0f6eb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_encoding",
    "resultHeight": 47
   },
   "source": [
    "##### Let's use the `OrdinalEncoder` to transform `COLOR` and `CLARITY` from categorical to numerical values so they are more meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf97705-c30b-48e9-b152-76e0ccc3a818",
   "metadata": {
    "language": "python",
    "name": "ordinal_encoding",
    "resultHeight": 797
   },
   "outputs": [],
   "source": [
    "# Encode CUT and CLARITY preserve ordinal importance\n",
    "categories = {\n",
    "    \"CUT\": np.array([\"IDEAL\", \"PREMIUM\", \"VERY_GOOD\", \"GOOD\", \"FAIR\"]),\n",
    "    \"CLARITY\": np.array([\"IF\", \"VVS1\", \"VVS2\", \"VS1\", \"VS2\", \"SI1\", \"SI2\", \"I1\", \"I2\", \"I3\"]),\n",
    "}\n",
    "snowml_oe = snowml.OrdinalEncoder(input_cols=[\"CUT\", \"CLARITY\"], output_cols=[\"CUT_OE\", \"CLARITY_OE\"], categories=categories)\n",
    "ord_encoded_diamonds_df = snowml_oe.fit(normalized_diamonds_df).transform(normalized_diamonds_df)\n",
    "\n",
    "# Show the encoding\n",
    "print(snowml_oe._state_pandas)\n",
    "\n",
    "ord_encoded_diamonds_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "412bfa35-f9dd-459a-99b3-5dd5d846f9e6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_encoder_2",
    "resultHeight": 166
   },
   "source": [
    "##### Let's use the `OneHotEncoder` to transform the categorical columns to numerical columns.\n",
    "\n",
    "This is more for illustration purposes. Using the OrdinalEncoder makes more sense for the diamonds dataset since `CARAT`, `COLOR`, and `CLARITY` all follow a natural ranking order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95781b03-1996-4171-8ebd-0a41f676a359",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "one_hot_encoding",
    "resultHeight": 439
   },
   "outputs": [],
   "source": [
    "# Encode categoricals to numeric columns\n",
    "snowml_ohe = snowml.OneHotEncoder(input_cols=[\"CUT\", \"COLOR\", \"CLARITY\"], output_cols=[\"CUT_OHE\", \"COLOR_OHE\", \"CLARITY_OHE\"])\n",
    "transformed_diamonds_df = snowml_ohe.fit(ord_encoded_diamonds_df).transform(ord_encoded_diamonds_df)\n",
    "\n",
    "np.array(transformed_diamonds_df.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "950944f6-de76-4820-a185-18feb4b24462",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_pipeline",
    "resultHeight": 99
   },
   "source": [
    "##### Finally, we can also build out a full preprocessing `Pipeline`.\n",
    "\n",
    "This will be useful for both the ML training & inference steps to have standarized feature transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cd00f7-46ac-46d4-804e-2894b01a1197",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "variables",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# Categorize all the features for processing\n",
    "CATEGORICAL_COLUMNS = [\"CUT\", \"COLOR\", \"CLARITY\"]\n",
    "CATEGORICAL_COLUMNS_OE = [\"CUT_OE\", \"COLOR_OE\", \"CLARITY_OE\"] # To name the ordinal encoded columns\n",
    "NUMERICAL_COLUMNS = [\"CARAT\", \"DEPTH\", \"TABLE_PCT\", \"X\", \"Y\", \"Z\"]\n",
    "\n",
    "categories = {\n",
    "    \"CUT\": np.array([\"IDEAL\", \"PREMIUM\", \"VERY_GOOD\", \"GOOD\", \"FAIR\"]),\n",
    "    \"CLARITY\": np.array([\"IF\", \"VVS1\", \"VVS2\", \"VS1\", \"VS2\", \"SI1\", \"SI2\", \"I1\", \"I2\", \"I3\"]),\n",
    "    \"COLOR\": np.array(['D', 'E', 'F', 'G', 'H', 'I', 'J']),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4082f2e2-a2ae-4858-9e4c-a0fd1540dd4d",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "build_pipeline",
    "resultHeight": 439
   },
   "outputs": [],
   "source": [
    "# Build the pipeline\n",
    "preprocessing_pipeline = Pipeline(\n",
    "    steps=[\n",
    "            (\n",
    "                \"OE\",\n",
    "                snowml.OrdinalEncoder(\n",
    "                    input_cols=CATEGORICAL_COLUMNS,\n",
    "                    output_cols=CATEGORICAL_COLUMNS_OE,\n",
    "                    categories=categories,\n",
    "                )\n",
    "            ),\n",
    "            (\n",
    "                \"MMS\",\n",
    "                snowml.MinMaxScaler(\n",
    "                    clip=True,\n",
    "                    input_cols=NUMERICAL_COLUMNS,\n",
    "                    output_cols=NUMERICAL_COLUMNS,\n",
    "                )\n",
    "            )\n",
    "    ]\n",
    ")\n",
    "\n",
    "PIPELINE_FILE = '/tmp/preprocessing_pipeline.joblib'\n",
    "joblib.dump(preprocessing_pipeline, PIPELINE_FILE) # We are just pickling it locally first\n",
    "\n",
    "transformed_diamonds_df = preprocessing_pipeline.fit(diamonds_df).transform(diamonds_df)\n",
    "transformed_diamonds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160f2c47-584a-4270-9c53-0a401eccc720",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "save_pipeline",
    "resultHeight": 354
   },
   "outputs": [],
   "source": [
    "# You can also save the pickled object into the stage we created earlier for deployment\n",
    "session.file.put(PIPELINE_FILE, \"@ML_HOL_ASSETS\", overwrite=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4bd6167-3f3f-4236-a962-fd5fd019bc12",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_explore_data",
    "resultHeight": 155
   },
   "source": [
    "### Data Exploration\n",
    "\n",
    "Now that we've transformed our features, let's calculate the correlation using Snowpark ML's `correlation()` function between each pair to better understand their relationships.\n",
    "\n",
    "*Note: Snowpark ML's pearson correlation function returns a Pandas DataFrame*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5fb644-2cf4-4721-82e4-311465d14ad2",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "calc_corr",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# corr_diamonds_df = correlation(df=transformed_diamonds_df)\n# corr_diamonds_df # This is a Pandas DataFrame"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d8c572-5119-4288-9d32-c42f157aaf37",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "viz_corr",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# # Generate a mask for the upper triangle\n# mask = np.triu(np.ones_like(corr_diamonds_df, dtype=bool))\n\n# # Create a heatmap with the features\n# plt.figure(figsize=(7, 7))\n# heatmap = sns.heatmap(corr_diamonds_df, mask=mask, cmap=\"YlGnBu\", annot=True, vmin=-1, vmax=1)"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba002b6c-9819-46cf-8b6a-7ff589150e49",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_plot_carat_price",
    "resultHeight": 109
   },
   "source": [
    "We note that `CARAT` and `PRICE` are highly correlated, which makes sense! Let's take a look at their relationship a bit closer.\n",
    "\n",
    "*Note: You will have to convert your Snowpark DF to a Pandas DF in order to use matplotlib & seaborn.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb10563-3acc-4db4-98fa-4e5b0dc9d553",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "plot_carat_price",
    "resultHeight": 1473
   },
   "outputs": [],
   "source": "# Set up a plot to look at CARAT and PRICE\ncounts = transformed_diamonds_df.to_pandas().groupby(['PRICE', 'CARAT', 'CLARITY_OE']).size().reset_index(name='Count')\n\nfig, ax = plt.subplots(figsize=(20, 20))\nplt.title('Price vs Carat', fontsize=28)\nax = sns.scatterplot(data=counts, x='CARAT', y='PRICE', size='Count', hue='CLARITY_OE', markers='o')\nax.grid(axis='y')\n\n# The relationship is not linear - it appears exponential which makes sense given the rarity of the large diamonds\nsns.move_legend(ax, \"upper left\")\nsns.despine(left=True, bottom=True)"
  },
  {
   "cell_type": "markdown",
   "id": "a29501c1-3653-4b51-81bb-439c35383db7",
   "metadata": {
    "collapsed": false,
    "name": "md_next_nb",
    "resultHeight": 67
   },
   "source": [
    "In the next notebook, we will look at how you can train an XGBoost model with the diamonds dataset using the Snowpark ML Modeling API."
   ]
  }
 ]
}