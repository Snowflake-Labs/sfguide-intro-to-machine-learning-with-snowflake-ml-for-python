{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7526bcca-bcc6-4d53-9225-54b8b8741d96",
   "metadata": {
    "name": "md_env",
    "collapsed": false,
    "resultHeight": 157
   },
   "source": "# IMPORTANT \nMake sure you've imported the [environment.yml](https://github.com/Snowflake-Labs/sfguide-intro-to-machine-learning-with-snowflake-ml-for-python/blob/main/notebooks/environment.yml) file provided in the git repo on the left sidebar.\n\nThis will ensure if you have the right packages needed to run this Notebook."
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88ad6fb5-6d82-47c0-b8f0-5470d4c393be",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_modeling",
    "resultHeight": 227
   },
   "source": [
    "## 3. ML Modeling\n",
    "\n",
    "- In this notebook, we will illustrate how to train an XGBoost model with the diamonds dataset using the [Snowpark ML Modeling API](https://docs.snowflake.com/en/developer-guide/snowpark-ml/snowpark-ml-modeling). \n",
    "- We also show how to do inference and manage models via Model Registry or as a UDF (See Appendix).\n",
    "\n",
    "The Snowpark ML Model API supports `scikit-learn`, `xgboost`, and `lightgbm` models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "efba96f4-156f-414d-9400-f72bcd8ddbd5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_import_libs",
    "resultHeight": 46
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b6df09-63fe-4038-b029-dfb5abd776c2",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "import_libs",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# Snowpark for Python\n",
    "from snowflake.snowpark.version import VERSION\n",
    "from snowflake.snowpark.functions import udf\n",
    "import snowflake.snowpark.functions as F\n",
    "\n",
    "# Snowpark ML\n",
    "from snowflake.ml.modeling.xgboost import XGBRegressor\n",
    "from snowflake.ml.modeling.model_selection import GridSearchCV\n",
    "from snowflake.ml.registry import Registry\n",
    "from snowflake.ml._internal.utils import identifier\n",
    "\n",
    "# data science libs\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from snowflake.ml.modeling.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# misc\n",
    "import json\n",
    "import joblib\n",
    "import cachetools\n",
    "\n",
    "# warning suppresion\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf79c3e-97e2-41d3-82c7-7e52a68c999f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "sql",
    "name": "init_sql",
    "codeCollapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": [
    "-- Using Warehouse, Database, and Schema created during Setup\n",
    "USE WAREHOUSE ML_HOL_WH;\n",
    "USE DATABASE ML_HOL_DB;\n",
    "USE SCHEMA ML_HOL_SCHEMA;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98602c72-c86c-4266-8515-6e0c3a6d9b8a",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "get_active_session",
    "resultHeight": 162
   },
   "outputs": [],
   "source": [
    "# Establish Secure Connection to Snowflake\n",
    "session = get_active_session()\n",
    "\n",
    "# Add a query tag to the session.\n",
    "session.query_tag = {\"origin\":\"sf_sit-is\", \n",
    "                     \"name\":\"e2e_ml_snowparkpython\", \n",
    "                     \"version\":{\"major\":1, \"minor\":0,},\n",
    "                     \"attributes\":{\"is_quickstart\":1}}\n",
    "session"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eaab22f2-c362-4ec5-85e1-103082e80c77",
   "metadata": {
    "name": "md_load_data",
    "collapsed": false,
    "resultHeight": 46
   },
   "source": [
    "### Load the data & preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff39875-9784-43d8-8135-35db94f64665",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "load_data",
    "resultHeight": 439
   },
   "outputs": [],
   "source": [
    "# Load in the data\n",
    "diamonds_df = session.table(\"DIAMONDS\")\n",
    "diamonds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca447267-d754-4f37-a888-ed3f950d112c",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "define_vars",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# Categorize all the features for modeling\n",
    "CATEGORICAL_COLUMNS = [\"CUT\", \"COLOR\", \"CLARITY\"]\n",
    "CATEGORICAL_COLUMNS_OE = [\"CUT_OE\", \"COLOR_OE\", \"CLARITY_OE\"] # To name the ordinal encoded columns\n",
    "NUMERICAL_COLUMNS = [\"CARAT\", \"DEPTH\", \"TABLE_PCT\", \"X\", \"Y\", \"Z\"]\n",
    "\n",
    "LABEL_COLUMNS = ['PRICE']\n",
    "OUTPUT_COLUMNS = ['PREDICTED_PRICE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3dcfe3-64a4-431a-8aed-49d77c849252",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "save_pipeline",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# Load the preprocessing pipeline object from stage- to do this, we download the preprocessing_pipeline.joblib.gz file to the warehouse\n",
    "# where our notebook is running, and then load it using joblib.\n",
    "session.file.get('@ML_HOL_ASSETS/preprocessing_pipeline.joblib.gz', '/tmp')\n",
    "PIPELINE_FILE = '/tmp/preprocessing_pipeline.joblib.gz'\n",
    "preprocessing_pipeline = joblib.load(PIPELINE_FILE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7384a5fa-086f-466c-900e-104e1302199e",
   "metadata": {
    "name": "md_model",
    "resultHeight": 482
   },
   "source": [
    "### Build a simple XGBoost Regression model\n",
    "\n",
    "What's happening here? \n",
    "\n",
    "- The `model.fit()` function actually creates a temporary stored procedure in the background. This also means that the model training is a single-node operation. Be sure to use a [Snowpark Optimized Warehouse](https://docs.snowflake.com/en/user-guide/warehouses-snowpark-optimized) if you need more memory. We are just using an XS Standard Virtual Warehouse here, which we created at the beginning of this quickstart.\n",
    "- The `model.predict()` function actually creates a temporary vectorized UDF in the background, which means the input DataFrame is batched as Pandas DataFrames and inference is parallelized across the batches of data.\n",
    "\n",
    "You can check the query history once you execute the following cell to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39853aa6-696d-4966-8cd9-ab4b10c4a0ab",
   "metadata": {
    "language": "python",
    "name": "train_test_split",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "diamonds_train_df, diamonds_test_df = diamonds_df.random_split(weights=[0.9, 0.1], seed=0)\n",
    "\n",
    "# Run the train and test sets through the Pipeline object we defined earlier\n",
    "train_df = preprocessing_pipeline.fit(diamonds_train_df).transform(diamonds_train_df)\n",
    "test_df = preprocessing_pipeline.transform(diamonds_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40828bb7-0541-4e21-ab0c-ca1c558c31d6",
   "metadata": {
    "language": "python",
    "name": "build_simple_model",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# Define the XGBRegressor\n",
    "regressor = XGBRegressor(\n",
    "    input_cols=CATEGORICAL_COLUMNS_OE+NUMERICAL_COLUMNS,\n",
    "    label_cols=LABEL_COLUMNS,\n",
    "    output_cols=OUTPUT_COLUMNS\n",
    ")\n",
    "\n",
    "# Train\n",
    "regressor.fit(train_df)\n",
    "\n",
    "# Predict\n",
    "result = regressor.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b096d6-0003-4409-860f-03ac960d7715",
   "metadata": {
    "language": "python",
    "name": "simple_predict",
    "resultHeight": 439
   },
   "outputs": [],
   "source": [
    "# Just to illustrate, we can also pass in a Pandas DataFrame to Snowpark ML's model.predict()\n",
    "regressor.predict(test_df[CATEGORICAL_COLUMNS_OE+NUMERICAL_COLUMNS].to_pandas())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4e53ef4-d99e-4c43-a1d5-4b14ec17cc5b",
   "metadata": {
    "name": "md_analyze_results",
    "resultHeight": 41
   },
   "source": [
    "Let's analyze the results using Snowpark ML's MAPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0710b88b-1698-4638-94a9-a3b1337c2b66",
   "metadata": {
    "language": "python",
    "name": "calc_mape",
    "resultHeight": 439
   },
   "outputs": [],
   "source": [
    "mape = mean_absolute_percentage_error(df=result, \n",
    "                                        y_true_col_names=\"PRICE\", \n",
    "                                        y_pred_col_names=\"PREDICTED_PRICE\")\n",
    "\n",
    "result.select(\"PRICE\", \"PREDICTED_PRICE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5974d66e-088d-45ec-8a68-a1c44d56d024",
   "metadata": {
    "language": "python",
    "name": "mape_val",
    "resultHeight": 38
   },
   "outputs": [],
   "source": [
    "print(f\"Mean absolute percentage error: {mape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572f437e-fbc0-4cb2-9066-e5755e3fe39a",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "plot_mape",
    "resultHeight": 994
   },
   "outputs": [],
   "source": [
    "# Plot actual vs predicted \n",
    "g = sns.relplot(data=result[\"PRICE\", \"PREDICTED_PRICE\"].to_pandas().astype(\"float64\"), x=\"PRICE\", y=\"PREDICTED_PRICE\", kind=\"scatter\")\n",
    "g.ax.axline((0,0), slope=1, color=\"r\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81db1e71-9f94-4ea0-82d9-4c49392cbb88",
   "metadata": {
    "name": "md_gridsearch",
    "resultHeight": 184
   },
   "source": [
    "### Now, let's use Snowpark ML's **Distributed** `GridSearchCV()` function to find optimal model parameters\n",
    "\n",
    "We will increase the warehouse size to scale up our hyperparameter tuning to take advantage of parallelized model training to accelerate this search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4523df-051b-47be-a3f4-6c17fd5923bf",
   "metadata": {
    "language": "sql",
    "name": "alter_wh_size",
    "resultHeight": 112
   },
   "outputs": [],
   "source": [
    "ALTER WAREHOUSE ML_HOL_WH SET WAREHOUSE_SIZE=LARGE;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d528112f-0be9-434a-83aa-2947e277a7db",
   "metadata": {
    "language": "python",
    "name": "run_gridsearch",
    "resultHeight": 1806,
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(\n",
    "    estimator=XGBRegressor(),\n",
    "    param_grid={\n",
    "        \"n_estimators\":[100, 200, 300, 400, 500],\n",
    "        \"learning_rate\":[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    },\n",
    "    n_jobs = -1,\n",
    "    scoring=\"neg_mean_absolute_percentage_error\",\n",
    "    input_cols=CATEGORICAL_COLUMNS_OE+NUMERICAL_COLUMNS,\n",
    "    label_cols=LABEL_COLUMNS,\n",
    "    output_cols=OUTPUT_COLUMNS\n",
    ")\n",
    "\n",
    "# Train\n",
    "grid_search.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8615da-dcb1-4def-91dc-a1afb97be9b6",
   "metadata": {
    "language": "sql",
    "name": "alter_wh_size_back",
    "resultHeight": 112,
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ALTER WAREHOUSE ML_HOL_WH SET WAREHOUSE_SIZE=XSMALL;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5eb1139f-08a8-4c9f-a210-dd617be50cd3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_best_estimator",
    "resultHeight": 135
   },
   "source": [
    "We see that the best estimator has the following parameters: `n_estimators=500` & `learning_rate=0.4`.\n",
    "\n",
    "We can use `to_sklearn()` in order to get the actual xgboost model object, which gives us access to all its attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09854ae0-24a4-4133-bf92-095d1abbeffb",
   "metadata": {
    "language": "python",
    "name": "get_best_estimator",
    "resultHeight": 262,
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(grid_search.to_sklearn().best_estimator_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de3dc8a0-aca1-496d-a76b-3dd165d88f19",
   "metadata": {
    "name": "md_analyze_grid_search",
    "resultHeight": 41
   },
   "source": [
    "We can also analyze the grid search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0600af1f-424d-49b9-be96-c01db0694444",
   "metadata": {
    "language": "python",
    "name": "analyze_grid_search",
    "resultHeight": 995,
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Analyze grid search results\n",
    "gs_results = grid_search.to_sklearn().cv_results_\n",
    "n_estimators_val = []\n",
    "learning_rate_val = []\n",
    "for param_dict in gs_results[\"params\"]:\n",
    "    n_estimators_val.append(param_dict[\"n_estimators\"])\n",
    "    learning_rate_val.append(param_dict[\"learning_rate\"])\n",
    "mape_val = gs_results[\"mean_test_score\"]*-1\n",
    "\n",
    "gs_results_df = pd.DataFrame(data={\n",
    "    \"n_estimators\":n_estimators_val,\n",
    "    \"learning_rate\":learning_rate_val,\n",
    "    \"mape\":mape_val})\n",
    "\n",
    "sns.relplot(data=gs_results_df, x=\"learning_rate\", y=\"mape\", hue=\"n_estimators\", kind=\"line\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e108b85-bf4f-4c0a-be83-66ab5d194415",
   "metadata": {
    "name": "md_best_estimator_params",
    "resultHeight": 67,
    "collapsed": false
   },
   "source": [
    "This is consistent with the `learning_rate=0.4` and `n_estimator=500` chosen as the best estimator with the lowest MAPE."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "422ff4ab-92ce-42e4-b3e9-92d8bc551ddc",
   "metadata": {
    "name": "md_predict_analyze_best",
    "resultHeight": 41
   },
   "source": [
    "Now, let's predict and analyze the results from using the best estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db69e45-b0a7-4d96-9417-781d1f6b3cc8",
   "metadata": {
    "language": "python",
    "name": "gridsearch_predict",
    "resultHeight": 367,
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "result = grid_search.predict(test_df)\n",
    "\n",
    "# Analyze results\n",
    "mape = mean_absolute_percentage_error(df=result, \n",
    "                                        y_true_col_names=\"PRICE\", \n",
    "                                        y_pred_col_names=\"PREDICTED_PRICE\")\n",
    "\n",
    "result.select(\"PRICE\", \"PREDICTED_PRICE\").show()\n",
    "print(f\"Mean absolute percentage error: {mape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07030287-f020-4f0e-8c5e-077781022633",
   "metadata": {
    "language": "python",
    "name": "gridsearch_plot",
    "resultHeight": 994,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot actual vs predicted \n",
    "g = sns.relplot(data=result[\"PRICE\", \"PREDICTED_PRICE\"].to_pandas().astype(\"float64\"), x=\"PRICE\", y=\"PREDICTED_PRICE\", kind=\"scatter\")\n",
    "g.ax.axline((0,0), slope=1, color=\"r\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1df12f-9783-48df-ae4c-6ed875a95a14",
   "metadata": {
    "name": "md_save_optimal_model",
    "resultHeight": 41,
    "collapsed": false
   },
   "source": [
    "Let's save our optimal model and its metadata:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b8f388-6a74-4907-ba8b-a1625eb0dc86",
   "metadata": {
    "language": "python",
    "name": "optimal_model_params",
    "resultHeight": 0,
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimal_model = grid_search.to_sklearn().best_estimator_\n",
    "optimal_n_estimators = grid_search.to_sklearn().best_estimator_.n_estimators\n",
    "optimal_learning_rate = grid_search.to_sklearn().best_estimator_.learning_rate\n",
    "\n",
    "optimal_mape = gs_results_df.loc[(gs_results_df['n_estimators']==optimal_n_estimators) &\n",
    "                                 (gs_results_df['learning_rate']==optimal_learning_rate), 'mape'].values[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ea2435e-bcfe-46ff-a630-872be3cb3622",
   "metadata": {
    "name": "md_model_registry",
    "collapsed": false,
    "resultHeight": 46
   },
   "source": [
    "### Manage models using Model Registry"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "905b456b-95c0-4e18-9eea-296c291eafaa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_model_registry_description",
    "resultHeight": 118
   },
   "source": [
    "Now, with Snowpark ML's [Model Registry](https://docs.snowflake.com/en/developer-guide/snowpark-ml/snowpark-ml-mlops-model-registry), we have a Snowflake native model versioning and deployment framework. This allows us to log models, tag parameters and metrics, track metadata, create versions, and ultimately execute batch inference tasks in a Snowflake warehouse or deploy to a Snowpark Container Service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab80ada-ff3d-402a-8abc-6b4849e552dc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_log_models",
    "resultHeight": 83
   },
   "source": [
    "First, we will log our models.\n",
    "\n",
    "Refer to [this Medium post](https://medium.com/snowflake/whats-in-a-name-model-naming-versioning-in-snowpark-model-registry-b5f7105fd6f6) on best practices for model naming & versioning conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a525afd-650f-443d-89ee-e2ff7024646c",
   "metadata": {
    "language": "python",
    "name": "log_models",
    "resultHeight": 0,
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "# Get sample input data to pass into the registry logging function\nX = train_df.select(CATEGORICAL_COLUMNS_OE+NUMERICAL_COLUMNS).limit(100)\n\ndb = identifier._get_unescaped_name(session.get_current_database())\nschema = identifier._get_unescaped_name(session.get_current_schema())\n\n# Define model name\nmodel_name = \"DIAMONDS_PRICE_PREDICTION\"\n\n# Create a registry and log the model\nnative_registry = Registry(session=session, database_name=db, schema_name=schema)\n\n# Let's first log the very first model we trained\nmodel_ver = native_registry.log_model(\n    model_name=model_name,\n    version_name='V0',\n    model=regressor,\n    sample_input_data=X, # to provide the feature schema\n    options={\"enable_explainability\": True}\n)\n\n# Add evaluation metric\nmodel_ver.set_metric(metric_name=\"mean_abs_pct_err\", value=mape)\n\n# Add a description\nmodel_ver.comment = \"This is the first iteration of our Diamonds Price Prediction model. It is used for demo purposes.\"\n\n# Now, let's log the optimal model from GridSearchCV\nmodel_ver2 = native_registry.log_model(\n    model_name=model_name,\n    version_name='V1',\n    model=optimal_model,\n    sample_input_data=X, # to provide the feature schema\n    options={\"enable_explainability\": True}\n)\n\n# Add evaluation metric\nmodel_ver2.set_metric(metric_name=\"mean_abs_pct_err\", value=optimal_mape)\n\n# Add a description\nmodel_ver2.comment = \"This is the second iteration of our Diamonds Price Prediction model \\\n                        where we performed hyperparameter optimization.\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c24f6d8-dc45-4e0b-bf17-8f1eb25104d3",
   "metadata": {
    "language": "python",
    "name": "get_logged_model",
    "codeCollapsed": false,
    "resultHeight": 147,
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's confirm they were added\n",
    "native_registry.get_model(model_name).show_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e139033-da10-4f13-959a-98538dcac547",
   "metadata": {
    "name": "see_default_model",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": [
    "We can see what the default model is when we have multiple versions with the same model name:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a209db8b-4e12-4866-9d5d-b8c063aca5cf",
   "metadata": {
    "language": "python",
    "name": "get_default_model",
    "codeCollapsed": false,
    "resultHeight": 54,
    "collapsed": false
   },
   "outputs": [],
   "source": "native_registry.get_model(model_name).default.version_name"
  },
  {
   "cell_type": "markdown",
   "id": "17a774c1-9ed2-4676-bcef-1a4bc99b27bd",
   "metadata": {
    "name": "md_optimal_model_inference",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": [
    "Now we can use the optimal model to perform inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf04908d-9cb9-4084-99c7-2b7905535587",
   "metadata": {
    "language": "python",
    "name": "run_inference",
    "codeCollapsed": false,
    "resultHeight": 351,
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_ver = native_registry.get_model(model_name).version('v1')\n",
    "result_sdf2 = model_ver.run(test_df, function_name=\"predict\")\n",
    "result_sdf2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b82429-87d5-41a6-beb6-f2787b37ebd8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_sql_inference",
    "resultHeight": 67
   },
   "source": [
    "You can also execute inference using SQL. To do this, we will use a SQL cell and reference our model's predict method via the model object's name, e.g. `DIAMONDS_PRICE_PREDICTION!predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c4c864-c5aa-4234-b2e9-8a7b8243638f",
   "metadata": {
    "language": "python",
    "name": "write_test_data",
    "resultHeight": 0,
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "test_df.write.mode('overwrite').save_as_table('DIAMONDS_TEST')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1312c300-b288-4f60-9554-305810c7e041",
   "metadata": {
    "language": "sql",
    "name": "sql_inference_1",
    "codeCollapsed": false,
    "resultHeight": 439,
    "collapsed": false
   },
   "outputs": [],
   "source": "--- for the default version:\nSELECT a.*, DIAMONDS_PRICE_PREDICTION!predict(a.CUT_OE, a.COLOR_OE, a.CLARITY_OE, a.CARAT, a.DEPTH, a.TABLE_PCT, a.X, a.Y, a.Z)['PREDICTED_PRICE'] as prediction from DIAMONDS_TEST a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646fb2f2-5358-4954-aa9e-dbaf4d4700e2",
   "metadata": {
    "language": "sql",
    "name": "sql_inference_2",
    "codeCollapsed": false,
    "resultHeight": 439,
    "collapsed": false
   },
   "outputs": [],
   "source": "--- for any other version (for example V1 below):\nWITH model_version_alias AS MODEL DIAMONDS_PRICE_PREDICTION VERSION v1 SELECT a.*, model_version_alias!predict(a.CUT_OE, a.COLOR_OE, a.CLARITY_OE, a.CARAT, a.DEPTH, a.TABLE_PCT, a.X, a.Y, a.Z)['output_feature_0'] as prediction from DIAMONDS_TEST a"
  },
  {
   "cell_type": "markdown",
   "id": "000acb48-0351-4acb-928b-230fe9b85a6e",
   "metadata": {
    "name": "model_explainability_md",
    "collapsed": false,
    "resultHeight": 308
   },
   "source": "### Model Explainability\n\nAnother thing we may want to look at to better understand the predictions are explanations on what the model considers most impactful when generating the predictions. To generate these explanations, we'll use the [built-in explainability function](https://docs.snowflake.com/en/developer-guide/snowflake-ml/model-registry/model-explainability) from Snowpark ML. \n\nUnder the hood, this function is based on [Shapley values](https://towardsdatascience.com/the-shapley-value-for-ml-models-f1100bff78d1). During the training process, machine learning models infer relationships between inputs and outputs, and Shapley values are a way to attribute the output of a machine learning model to its input features. By considering all possible combinations of features, Shapley values measure the average marginal contribution of each feature to the modelâ€™s prediction. While computationally intensive, the insights gained from Shapley values are invaluable for model interpretability and debugging."
  },
  {
   "cell_type": "markdown",
   "id": "a7d996c9-bcef-4193-86a3-dc9fee7d60e7",
   "metadata": {
    "name": "calc_explain_md",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "Let's calculate these explanations based on our optimal model now."
  },
  {
   "cell_type": "code",
   "id": "a0f788a0-197c-4b1e-90d8-543a8776b5f4",
   "metadata": {
    "language": "python",
    "name": "python_explain",
    "collapsed": false,
    "codeCollapsed": false,
    "resultHeight": 439
   },
   "outputs": [],
   "source": "mv_explanations = model_ver.run(train_df, function_name=\"explain\")\nmv_explanations",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fccd86df-ce0a-45c1-a8e2-ceda9068b4c9",
   "metadata": {
    "name": "viz_explanation_md",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "Let's visualize these explanations since it's a bit hard to just interpret the values themselves."
  },
  {
   "cell_type": "code",
   "id": "b9eef633-252a-46e9-ba0d-05edfc28a8d6",
   "metadata": {
    "language": "python",
    "name": "python_shap",
    "codeCollapsed": false,
    "resultHeight": 464,
    "collapsed": false
   },
   "outputs": [],
   "source": "import shap\n\nmv_explanations_pd = mv_explanations.to_pandas()\n\nmv_explanations_pd_2 = mv_explanations_pd[['CUT_OE_explanation',\n                                           'COLOR_OE_explanation',\n                                           'CLARITY_OE_explanation',\n                                           'CARAT_explanation',\n                                           'DEPTH_explanation',\n                                           'TABLE_PCT_explanation',\n                                           'X_explanation',\n                                           'Y_explanation',\n                                           'Z_explanation']]\n\n# Wrapping the explanations DataFrame into a SHAP recognized object\nshap_exp = shap._explanation.Explanation(mv_explanations_pd_2.values, \n                                         feature_names = mv_explanations_pd_2.columns)\nshap.plots.bar(shap_exp)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cfb01b0e-c4bf-49f0-867b-9b38c43c1680",
   "metadata": {
    "name": "explanation_plot_md",
    "collapsed": false,
    "resultHeight": 94
   },
   "source": "We see that `CARAT` has the biggest impact on the prediction values (`PRICE`) followed by the `Y dimension`, `CLARITY`, and `COLOR`. This is what we observed in the data exploration phase in the previous notebook too when plotting `PRICE vs CARAT`."
  },
  {
   "cell_type": "markdown",
   "id": "734e0196-8a52-4664-9d93-35a5ddb5e9bb",
   "metadata": {
    "name": "save_train_df_md",
    "collapsed": false,
    "resultHeight": 67
   },
   "source": "Let's save our training data into a Snowflake table to illustrate how the SQL API version of this function can be also be used to generate feature explanations."
  },
  {
   "cell_type": "code",
   "id": "1d022983-f221-4a0a-8379-d96ce35ab07d",
   "metadata": {
    "language": "python",
    "name": "save_train_df",
    "codeCollapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "train_df.write.mode('overwrite').save_as_table('DIAMONDS_TRAIN')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c910b1ab-1e6f-4447-8741-ac11d2051f0e",
   "metadata": {
    "name": "sql_explain_md",
    "collapsed": false,
    "resultHeight": 67
   },
   "source": "Now, we can call the SQL API by calling the model and the version we want to evaluate and generate those explanations."
  },
  {
   "cell_type": "code",
   "id": "3c176234-e138-40b6-b6c6-4fdacd5172cd",
   "metadata": {
    "language": "sql",
    "name": "sql_explain",
    "codeCollapsed": false,
    "resultHeight": 511,
    "collapsed": false
   },
   "outputs": [],
   "source": "WITH mv AS MODEL \"DIAMONDS_PRICE_PREDICTION\" VERSION \"V1\"\nSELECT * FROM DIAMONDS_TRAIN,\n  TABLE(mv!\"EXPLAIN\"(\n    CUT_OE,\n    COLOR_OE,\n    CLARITY_OE,\n    CARAT,\n    DEPTH,\n    TABLE_PCT,\n    X,\n    Y,\n    Z\n  ));",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ad1ef059-c310-4dcf-9305-99fe17173820",
   "metadata": {
    "name": "md_clean_up",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": [
    "Let's do some clean up now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbcedb9-712c-4da7-8a52-f1b645fa1c1d",
   "metadata": {
    "language": "python",
    "name": "delete_model",
    "codeCollapsed": false,
    "resultHeight": 0,
    "collapsed": false
   },
   "outputs": [],
   "source": "# Clean up\nnative_registry.delete_model(model_name)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b67c88-8b26-4a81-a9c1-8d9f08506e13",
   "metadata": {
    "language": "python",
    "name": "show_models",
    "codeCollapsed": false,
    "resultHeight": 112,
    "collapsed": false
   },
   "outputs": [],
   "source": "# Confirm it was deleted\nnative_registry.show_models()"
  },
  {
   "cell_type": "markdown",
   "id": "161c5609-8ec5-41e0-90df-5f43fb87e063",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_conclusion",
    "resultHeight": 481
   },
   "source": [
    "## Conclusion\n",
    "Congratulations, you have successfully completed this quickstart! Through this quickstart, we were able to showcase Snowpark for Machine Learning through the introduction of Snowpark ML, the Python library and underlying infrastructure for data science and machine learning tasks. Now, you can run data preprocessing, feature engineering, model training, and batch inference in a few lines of code without having to define and deploy stored procedures that package scikit-learn, xgboost, or lightgbm code.\n",
    "\n",
    "For more information, check out the resources below:\n",
    "\n",
    "### Related Resources\n",
    "- [Snowpark ML API Docs](https://docs.snowflake.com/en/developer-guide/snowpark-ml/index)\n",
    "- [Getting Started with Data Engineering and ML Using Snowpark](https://quickstarts.snowflake.com/guide/getting_started_with_dataengineering_ml_using_snowpark_python/index.html?index=..%2F..index#0)\n",
    "- [Advanced: Snowpark for Python Data Engineering Guide](https://quickstarts.snowflake.com/guide/data_engineering_pipelines_with_snowpark_python/index.html)\n",
    "- [Advanced: Snowpark for Python Machine Learning Guide](https://quickstarts.snowflake.com/guide/getting_started_snowpark_machine_learning/index.html)\n",
    "- [Snowpark for Python Demos](https://github.com/Snowflake-Labs/snowpark-python-demos/blob/main/README.md)\n",
    "- [Snowpark for Python Developer Docs](https://docs.snowflake.com/en/developer-guide/snowpark/python/index.html)\n"
   ]
  }
 ]
}