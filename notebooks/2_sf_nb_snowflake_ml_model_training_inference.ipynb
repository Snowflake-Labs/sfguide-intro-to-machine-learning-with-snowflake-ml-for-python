{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "lastEditStatus": {
   "notebookId": "nujhnuijpguz4mqa5cd6",
   "authorId": "158808794318",
   "authorName": "SIKHADAS",
   "authorEmail": "sikha.das@snowflake.com",
   "sessionId": "d539baea-b19f-4c41-8781-c0aa8b335469",
   "lastEditTime": 1747170577318
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88ad6fb5-6d82-47c0-b8f0-5470d4c393be",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_modeling",
    "resultHeight": 227
   },
   "source": "## 3. ML Modeling\n\n- In this notebook, we will illustrate how to train an XGBoost model with the diamonds dataset using [OSS XGBoost](https://xgboost.readthedocs.io/en). \n- We also show how to do inference and manage models via Model Registry."
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "efba96f4-156f-414d-9400-f72bcd8ddbd5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_import_libs",
    "resultHeight": 46
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "id": "20941b81-886f-4aa7-9079-5aa39025be35",
   "metadata": {
    "language": "python",
    "name": "shap_install"
   },
   "outputs": [],
   "source": "!pip install shap",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b6df09-63fe-4038-b029-dfb5abd776c2",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "import_libs",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# Snowpark for Python\nfrom snowflake.snowpark.version import VERSION\nimport snowflake.snowpark.functions as F\n\n# Snowflake ML\nfrom snowflake.ml.registry import Registry\nfrom snowflake.ml._internal.utils import identifier\n\n# data science libs\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.model_selection import GridSearchCV\n\n# misc\nimport json\nimport joblib\nimport cachetools\n\n# warning suppresion\nimport warnings; warnings.simplefilter('ignore')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf79c3e-97e2-41d3-82c7-7e52a68c999f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "sql",
    "name": "init_sql",
    "codeCollapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": [
    "-- Using Warehouse, Database, and Schema created during Setup\n",
    "USE WAREHOUSE ML_HOL_WH;\n",
    "USE DATABASE ML_HOL_DB;\n",
    "USE SCHEMA ML_HOL_SCHEMA;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98602c72-c86c-4266-8515-6e0c3a6d9b8a",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "get_active_session",
    "resultHeight": 162
   },
   "outputs": [],
   "source": [
    "# Establish Secure Connection to Snowflake\n",
    "session = get_active_session()\n",
    "\n",
    "# Add a query tag to the session.\n",
    "session.query_tag = {\"origin\":\"sf_sit-is\", \n",
    "                     \"name\":\"e2e_ml_snowparkpython\", \n",
    "                     \"version\":{\"major\":1, \"minor\":0,},\n",
    "                     \"attributes\":{\"is_quickstart\":1}}\n",
    "session"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eaab22f2-c362-4ec5-85e1-103082e80c77",
   "metadata": {
    "name": "md_load_data",
    "collapsed": false,
    "resultHeight": 46
   },
   "source": [
    "### Load the data & preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff39875-9784-43d8-8135-35db94f64665",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "load_data",
    "resultHeight": 439
   },
   "outputs": [],
   "source": [
    "# Load in the data\n",
    "diamonds_df = session.table(\"DIAMONDS\")\n",
    "diamonds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca447267-d754-4f37-a888-ed3f950d112c",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "define_vars",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# Categorize all the features for modeling\n",
    "CATEGORICAL_COLUMNS = [\"CUT\", \"COLOR\", \"CLARITY\"]\n",
    "CATEGORICAL_COLUMNS_OE = [\"CUT_OE\", \"COLOR_OE\", \"CLARITY_OE\"] # To name the ordinal encoded columns\n",
    "NUMERICAL_COLUMNS = [\"CARAT\", \"DEPTH\", \"TABLE_PCT\", \"X\", \"Y\", \"Z\"]\n",
    "\n",
    "LABEL_COLUMNS = ['PRICE']\n",
    "OUTPUT_COLUMNS = ['PREDICTED_PRICE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3dcfe3-64a4-431a-8aed-49d77c849252",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "save_pipeline",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# Load the preprocessing pipeline object from stage- to do this, we download the preprocessing_pipeline.joblib.gz file to the warehouse\n",
    "# where our notebook is running, and then load it using joblib.\n",
    "session.file.get('@ML_HOL_ASSETS/preprocessing_pipeline.joblib.gz', '/tmp')\n",
    "PIPELINE_FILE = '/tmp/preprocessing_pipeline.joblib.gz'\n",
    "preprocessing_pipeline = joblib.load(PIPELINE_FILE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7384a5fa-086f-466c-900e-104e1302199e",
   "metadata": {
    "name": "md_model",
    "resultHeight": 482,
    "collapsed": false
   },
   "source": "### Build a simple open-source XGBoost Regression model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39853aa6-696d-4966-8cd9-ab4b10c4a0ab",
   "metadata": {
    "language": "python",
    "name": "train_test_split",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# Split the data into train and test sets\ndiamonds_train_df, diamonds_test_df = diamonds_df.random_split(weights=[0.9, 0.1], seed=0)\n\n# Run the train and test sets through the Pipeline object we defined earlier\ntrain_df = preprocessing_pipeline.fit(diamonds_train_df).transform(diamonds_train_df)\ntest_df = preprocessing_pipeline.transform(diamonds_test_df)\n\n# Convert to pandas dataframes to use OSS XGBoost\ntrain_pd = train_df.select(CATEGORICAL_COLUMNS_OE+NUMERICAL_COLUMNS+LABEL_COLUMNS).to_pandas()\ntest_pd = test_df.select(CATEGORICAL_COLUMNS_OE+NUMERICAL_COLUMNS+LABEL_COLUMNS).to_pandas()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40828bb7-0541-4e21-ab0c-ca1c558c31d6",
   "metadata": {
    "language": "python",
    "name": "build_simple_model",
    "resultHeight": 0,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Define model config\nregressor = XGBRegressor()\n\n# Split train data into X, y\ny_train_pd = train_pd.PRICE\nX_train_pd = train_pd.drop(columns=['PRICE'])\n\n# Train model\nregressor.fit(X_train_pd, y_train_pd)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b096d6-0003-4409-860f-03ac960d7715",
   "metadata": {
    "language": "python",
    "name": "simple_predict",
    "resultHeight": 439,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# We can now get predictions\ny_test_pred = regressor.predict(test_pd.drop(columns=['PRICE']))\ny_train_pred = regressor.predict(train_pd.drop(columns=['PRICE']))"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4e53ef4-d99e-4c43-a1d5-4b14ec17cc5b",
   "metadata": {
    "name": "md_analyze_results",
    "resultHeight": 41,
    "collapsed": false
   },
   "source": [
    "Let's analyze the results using Snowflake ML's MAPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0710b88b-1698-4638-94a9-a3b1337c2b66",
   "metadata": {
    "language": "python",
    "name": "calc_mape",
    "resultHeight": 439
   },
   "outputs": [],
   "source": "mape = mean_absolute_percentage_error(y_train_pd, y_train_pred)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5974d66e-088d-45ec-8a68-a1c44d56d024",
   "metadata": {
    "language": "python",
    "name": "print_mape_val",
    "resultHeight": 38,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "print(f\"Mean absolute percentage error: {mape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81db1e71-9f94-4ea0-82d9-4c49392cbb88",
   "metadata": {
    "name": "md_gridsearch",
    "resultHeight": 184,
    "collapsed": false
   },
   "source": "### Now, let's use `scikit-learn`'s `GridSearchCV` function to find optimal model parameters"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d528112f-0be9-434a-83aa-2947e277a7db",
   "metadata": {
    "language": "python",
    "name": "run_gridsearch",
    "resultHeight": 1806,
    "collapsed": false
   },
   "outputs": [],
   "source": "parameters={\n        \"n_estimators\":[100, 200, 500],\n        \"learning_rate\":[0.1, 0.4]\n}\n\nxgb = XGBRegressor()\nclf = GridSearchCV(xgb, parameters)\nclf.fit(X_train_pd, y_train_pd)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09854ae0-24a4-4133-bf92-095d1abbeffb",
   "metadata": {
    "language": "python",
    "name": "get_best_estimator",
    "resultHeight": 262,
    "collapsed": false
   },
   "outputs": [],
   "source": "print(clf.best_estimator_)"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5eb1139f-08a8-4c9f-a210-dd617be50cd3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_best_estimator",
    "resultHeight": 135
   },
   "source": "We see that the best estimator has the following parameters: `n_estimators=500` & `learning_rate=0.4`."
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de3dc8a0-aca1-496d-a76b-3dd165d88f19",
   "metadata": {
    "name": "md_analyze_grid_search",
    "resultHeight": 41,
    "collapsed": false
   },
   "source": "We can also analyze the full grid search results."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0600af1f-424d-49b9-be96-c01db0694444",
   "metadata": {
    "language": "python",
    "name": "analyze_grid_search",
    "resultHeight": 995,
    "collapsed": false
   },
   "outputs": [],
   "source": "# Analyze grid search results\ngs_results = clf.cv_results_\nn_estimators_val = []\nlearning_rate_val = []\nfor param_dict in gs_results[\"params\"]:\n    n_estimators_val.append(param_dict[\"n_estimators\"])\n    learning_rate_val.append(param_dict[\"learning_rate\"])\nmape_val = gs_results[\"mean_test_score\"]*-1\n\ngs_results_df = pd.DataFrame(data={\n    \"n_estimators\":n_estimators_val,\n    \"learning_rate\":learning_rate_val,\n    \"mape\":mape_val})\n\nsns.relplot(data=gs_results_df, x=\"learning_rate\", y=\"mape\", hue=\"n_estimators\", kind=\"line\")\n\nplt.show()"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e108b85-bf4f-4c0a-be83-66ab5d194415",
   "metadata": {
    "name": "md_best_estimator_params",
    "resultHeight": 67,
    "collapsed": false
   },
   "source": [
    "This is consistent with the `learning_rate=0.4` and `n_estimator=500` chosen as the best estimator with the lowest MAPE."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "422ff4ab-92ce-42e4-b3e9-92d8bc551ddc",
   "metadata": {
    "name": "md_predict_analyze_best",
    "resultHeight": 41
   },
   "source": [
    "Now, let's predict and analyze the results from using the best estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db69e45-b0a7-4d96-9417-781d1f6b3cc8",
   "metadata": {
    "language": "python",
    "name": "gridsearch_predict",
    "resultHeight": 367,
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from sklearn.metrics import mean_absolute_percentage_error\n\n# Predict\nopt_model = clf.best_estimator_\ny_train_pred = opt_model.predict(train_pd.drop(columns=['PRICE']))\n\nmape = mean_absolute_percentage_error(y_train_pd, y_train_pred)\n\nprint(f\"Mean absolute percentage error: {mape}\")"
  },
  {
   "cell_type": "markdown",
   "id": "de1df12f-9783-48df-ae4c-6ed875a95a14",
   "metadata": {
    "name": "md_save_optimal_model",
    "resultHeight": 41,
    "collapsed": false
   },
   "source": [
    "Let's save our optimal model and its metadata:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b8f388-6a74-4907-ba8b-a1625eb0dc86",
   "metadata": {
    "language": "python",
    "name": "optimal_model_params",
    "resultHeight": 0,
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "optimal_model = clf.best_estimator_\noptimal_n_estimators = clf.best_estimator_.n_estimators\noptimal_learning_rate = clf.best_estimator_.learning_rate\n\noptimal_mape = gs_results_df.loc[(gs_results_df['n_estimators']==optimal_n_estimators) &\n                                 (gs_results_df['learning_rate']==optimal_learning_rate), 'mape'].values[0]"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ea2435e-bcfe-46ff-a630-872be3cb3622",
   "metadata": {
    "name": "md_model_registry",
    "collapsed": false,
    "resultHeight": 46
   },
   "source": [
    "### Manage models using Model Registry"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "905b456b-95c0-4e18-9eea-296c291eafaa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_model_registry_description",
    "resultHeight": 118
   },
   "source": [
    "Now, with Snowflake ML's [Model Registry](https://docs.snowflake.com/en/developer-guide/snowpark-ml/snowpark-ml-mlops-model-registry), we have a Snowflake native model versioning and deployment framework. This allows us to log models, tag parameters and metrics, track metadata, create versions, and ultimately execute batch inference tasks in a Snowflake warehouse or deploy to a Snowpark Container Service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab80ada-ff3d-402a-8abc-6b4849e552dc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_log_models",
    "resultHeight": 83
   },
   "source": [
    "First, we will log our models.\n",
    "\n",
    "Refer to [this Medium post](https://medium.com/snowflake/whats-in-a-name-model-naming-versioning-in-snowpark-model-registry-b5f7105fd6f6) on best practices for model naming & versioning conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a525afd-650f-443d-89ee-e2ff7024646c",
   "metadata": {
    "language": "python",
    "name": "log_models",
    "resultHeight": 0,
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "# Get sample input data to pass into the registry logging function\nX = train_df.select(CATEGORICAL_COLUMNS_OE+NUMERICAL_COLUMNS).limit(100)\n\ndb = identifier._get_unescaped_name(session.get_current_database())\nschema = identifier._get_unescaped_name(session.get_current_schema())\n\n# Define model name\nmodel_name = \"DIAMONDS_PRICE_PREDICTION\"\n\n# Create a registry and log the model\nnative_registry = Registry(session=session, database_name=db, schema_name=schema)\n\n# Let's first log the very first model we trained\nmodel_ver = native_registry.log_model(\n    model_name=model_name,\n    version_name='V0',\n    model=regressor,\n    sample_input_data=X, # to provide the feature schema\n    target_platforms={'WAREHOUSE'}\n)\n\n# Add evaluation metric\nmodel_ver.set_metric(metric_name=\"mean_abs_pct_err\", value=mape)\n\n# Add a description\nmodel_ver.comment = \"This is the first iteration of our Diamonds Price Prediction model. It is used for demo purposes.\"\n\n# Now, let's log the optimal model from GridSearchCV\nmodel_ver2 = native_registry.log_model(\n    model_name=model_name,\n    version_name='V1',\n    model=optimal_model,\n    sample_input_data=X, # to provide the feature schema\n    target_platforms={'WAREHOUSE'}\n)\n\n# Add evaluation metric\nmodel_ver2.set_metric(metric_name=\"mean_abs_pct_err\", value=optimal_mape)\n\n# Add a description\nmodel_ver2.comment = f\"This is the second iteration of our Diamonds Price Prediction model \\\n                        where we performed hyperparameter optimization. \\\n                        Optimal n_estimators & learning_rate: {optimal_n_estimators}, {optimal_learning_rate}\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c24f6d8-dc45-4e0b-bf17-8f1eb25104d3",
   "metadata": {
    "language": "python",
    "name": "get_logged_model",
    "codeCollapsed": false,
    "resultHeight": 147,
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's confirm they were added\n",
    "native_registry.get_model(model_name).show_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e139033-da10-4f13-959a-98538dcac547",
   "metadata": {
    "name": "see_default_model",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": [
    "We can see what the default model is when we have multiple versions with the same model name:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a209db8b-4e12-4866-9d5d-b8c063aca5cf",
   "metadata": {
    "language": "python",
    "name": "get_default_model",
    "codeCollapsed": false,
    "resultHeight": 54,
    "collapsed": false
   },
   "outputs": [],
   "source": "native_registry.get_model(model_name).default.version_name"
  },
  {
   "cell_type": "markdown",
   "id": "17a774c1-9ed2-4676-bcef-1a4bc99b27bd",
   "metadata": {
    "name": "md_optimal_model_inference",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": [
    "Now we can use the optimal model to perform inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf04908d-9cb9-4084-99c7-2b7905535587",
   "metadata": {
    "language": "python",
    "name": "run_inference",
    "codeCollapsed": false,
    "resultHeight": 351,
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_ver = native_registry.get_model(model_name).version('v1')\n",
    "result_sdf2 = model_ver.run(test_df, function_name=\"predict\")\n",
    "result_sdf2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b82429-87d5-41a6-beb6-f2787b37ebd8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_sql_inference",
    "resultHeight": 67
   },
   "source": "You can also execute inference using SQL. To do this, we will use a SQL cell and reference our model's predict method via the model object's name."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c4c864-c5aa-4234-b2e9-8a7b8243638f",
   "metadata": {
    "language": "python",
    "name": "write_test_data",
    "resultHeight": 0,
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "test_df.write.mode('overwrite').save_as_table('DIAMONDS_TEST')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646fb2f2-5358-4954-aa9e-dbaf4d4700e2",
   "metadata": {
    "language": "sql",
    "name": "sql_inference_2",
    "codeCollapsed": false,
    "resultHeight": 439,
    "collapsed": false
   },
   "outputs": [],
   "source": "--- for any other version (for example V1 below):\nWITH model_version_alias AS MODEL DIAMONDS_PRICE_PREDICTION VERSION v1 SELECT a.*, model_version_alias!predict(a.CUT_OE, a.COLOR_OE, a.CLARITY_OE, a.CARAT, a.DEPTH, a.TABLE_PCT, a.X, a.Y, a.Z)['output_feature_0'] as prediction from DIAMONDS_TEST a"
  },
  {
   "cell_type": "markdown",
   "id": "000acb48-0351-4acb-928b-230fe9b85a6e",
   "metadata": {
    "name": "model_explainability_md",
    "collapsed": false,
    "resultHeight": 308
   },
   "source": "### Model Explainability\n\nAnother thing we may want to look at to better understand the predictions are explanations on what the model considers most impactful when generating the predictions. To generate these explanations, we'll use the [built-in explainability function](https://docs.snowflake.com/en/developer-guide/snowflake-ml/model-registry/model-explainability) from Snowflake ML. \n\nUnder the hood, this function is based on [Shapley values](https://towardsdatascience.com/the-shapley-value-for-ml-models-f1100bff78d1). During the training process, machine learning models infer relationships between inputs and outputs, and Shapley values are a way to attribute the output of a machine learning model to its input features. By considering all possible combinations of features, Shapley values measure the average marginal contribution of each feature to the modelâ€™s prediction. While computationally intensive, the insights gained from Shapley values are invaluable for model interpretability and debugging."
  },
  {
   "cell_type": "markdown",
   "id": "a7d996c9-bcef-4193-86a3-dc9fee7d60e7",
   "metadata": {
    "name": "calc_explain_md",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "Let's calculate these explanations based on our optimal model now."
  },
  {
   "cell_type": "code",
   "id": "a0f788a0-197c-4b1e-90d8-543a8776b5f4",
   "metadata": {
    "language": "python",
    "name": "python_explain",
    "collapsed": false,
    "codeCollapsed": false,
    "resultHeight": 439
   },
   "outputs": [],
   "source": "mv_explanations = model_ver.run(train_df, function_name=\"explain\")\nmv_explanations",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fccd86df-ce0a-45c1-a8e2-ceda9068b4c9",
   "metadata": {
    "name": "viz_explanation_md",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "Let's visualize these explanations since it's a bit hard to just interpret the values themselves."
  },
  {
   "cell_type": "code",
   "id": "b9eef633-252a-46e9-ba0d-05edfc28a8d6",
   "metadata": {
    "language": "python",
    "name": "python_shap",
    "codeCollapsed": false,
    "resultHeight": 464,
    "collapsed": false
   },
   "outputs": [],
   "source": "import shap\n\n# Create a sample of 1000 records\ntest_pd = test_df.to_pandas()\ntest_pd_sample = test_pd.sample(n=1000, random_state = 100).reset_index(drop=True)\n\n# Compute shapley values for each model\nshap_pd = model_ver.run(test_pd_sample, function_name=\"explain\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cfb01b0e-c4bf-49f0-867b-9b38c43c1680",
   "metadata": {
    "name": "explanation_plot_md",
    "collapsed": false,
    "resultHeight": 94
   },
   "source": "We see that `CARAT` has the biggest impact on the prediction values (`PRICE`) followed by the `Y dimension`, `CLARITY`, and `COLOR`. This is what we observed in the data exploration phase in the previous notebook too when plotting `PRICE vs CARAT`."
  },
  {
   "cell_type": "markdown",
   "id": "734e0196-8a52-4664-9d93-35a5ddb5e9bb",
   "metadata": {
    "name": "save_train_df_md",
    "collapsed": false,
    "resultHeight": 67
   },
   "source": "Let's save our training data into a Snowflake table to illustrate how the SQL API version of this function can be also be used to generate feature explanations."
  },
  {
   "cell_type": "code",
   "id": "1d022983-f221-4a0a-8379-d96ce35ab07d",
   "metadata": {
    "language": "python",
    "name": "save_train_df",
    "codeCollapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "train_df.write.mode('overwrite').save_as_table('DIAMONDS_TRAIN')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c910b1ab-1e6f-4447-8741-ac11d2051f0e",
   "metadata": {
    "name": "sql_explain_md",
    "collapsed": false,
    "resultHeight": 67
   },
   "source": "Now, we can call the SQL API by calling the model and the version we want to evaluate and generate those explanations."
  },
  {
   "cell_type": "code",
   "id": "3c176234-e138-40b6-b6c6-4fdacd5172cd",
   "metadata": {
    "language": "sql",
    "name": "sql_explain",
    "codeCollapsed": false,
    "resultHeight": 511,
    "collapsed": false
   },
   "outputs": [],
   "source": "WITH mv AS MODEL \"DIAMONDS_PRICE_PREDICTION\" VERSION \"V1\"\nSELECT * FROM DIAMONDS_TRAIN,\n  TABLE(mv!\"EXPLAIN\"(\n    CUT_OE,\n    COLOR_OE,\n    CLARITY_OE,\n    CARAT,\n    DEPTH,\n    TABLE_PCT,\n    X,\n    Y,\n    Z\n  ));",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ad1ef059-c310-4dcf-9305-99fe17173820",
   "metadata": {
    "name": "md_clean_up",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": [
    "Let's do some clean up now. **UNCOMMENT THE FOLLOWING LINES TO DELETE THE MODEL BEFORE RE-RUNNING THIS NOTEBOOK. Don't delete the model if you plan to set up the Streamlit app.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbcedb9-712c-4da7-8a52-f1b645fa1c1d",
   "metadata": {
    "language": "python",
    "name": "delete_model",
    "codeCollapsed": false,
    "resultHeight": 0,
    "collapsed": false
   },
   "outputs": [],
   "source": "# Clean up\n#native_registry.delete_model(model_name)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b67c88-8b26-4a81-a9c1-8d9f08506e13",
   "metadata": {
    "language": "python",
    "name": "show_models",
    "codeCollapsed": false,
    "resultHeight": 112,
    "collapsed": false
   },
   "outputs": [],
   "source": "# Confirm it was deleted\n#native_registry.show_models()"
  }
 ]
}
