{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92e32af2-b6bf-4086-a361-93e61b633d44",
   "metadata": {
    "name": "md_env",
    "collapsed": false,
    "resultHeight": 157
   },
   "source": "# IMPORTANT \nMake sure you've imported the [environment.yml](https://github.com/Snowflake-Labs/sfguide-intro-to-machine-learning-with-snowflake-ml-for-python/blob/main/notebooks/environment.yml) file provided in the git repo on the left sidebar.\n\nThis will ensure if you have the right packages needed to run this Notebook."
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f4adaa3-8ef6-4c8f-8057-1d12bf900962",
   "metadata": {
    "name": "md_data_ingest",
    "resultHeight": 179,
    "collapsed": false
   },
   "source": [
    "## 1. Data Ingestion\n",
    "\n",
    "The `diamonds` dataset has been widely used in data science and machine learning. We will use it to demonstrate Snowflake's native data science transformers in terms of database functionality and Spark & Pandas comportablity, using non-synthetic and statistically appropriate data that is well known to the ML community.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6fe2a4b4-821f-410c-8d2c-e527829b106e",
   "metadata": {
    "name": "md_import_libs",
    "resultHeight": 46
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eed4494-06ab-43b2-86e8-879c99c1a2c0",
   "metadata": {
    "language": "python",
    "name": "import_libs",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# Snowpark for Python\n",
    "from snowflake.snowpark.types import DoubleType\n",
    "import snowflake.snowpark.functions as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39326625-0d17-438f-a847-8d1e2c1488da",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_active_session",
    "resultHeight": 113
   },
   "source": [
    "### Setup and establish Secure Connection to Snowflake\n",
    "\n",
    "Notebooks establish a Snowpark Session when the notebook is attached to the kernel. We create a new warehouse, database, and schema that will be used throughout this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76605036-bbf0-45cd-ac16-8c7bb391384c",
   "metadata": {
    "language": "sql",
    "name": "init_sql",
    "resultHeight": 87
   },
   "outputs": [],
   "source": [
    "-- Using Warehouse, Database, and Schema created during Setup\n",
    "USE WAREHOUSE ML_HOL_WH;\n",
    "USE DATABASE ML_HOL_DB;\n",
    "USE SCHEMA ML_HOL_SCHEMA;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046fa3ea-5ea9-4af6-a4dd-88c7101dcf0d",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "get_active_session",
    "resultHeight": 150
   },
   "outputs": [],
   "source": [
    "# Get Snowflake Session object\n",
    "session = get_active_session()\n",
    "session.sql_simplifier_enabled = True\n",
    "\n",
    "# Add a query tag to the session.\n",
    "session.query_tag = {\"origin\":\"sf_sit-is\", \n",
    "                     \"name\":\"e2e_ml_snowparkpython\", \n",
    "                     \"version\":{\"major\":1, \"minor\":0,},\n",
    "                     \"attributes\":{\"is_quickstart\":1}}\n",
    "\n",
    "# Current Environment Details\n",
    "print('Connection Established with the following parameters:')\n",
    "print('User      : {}'.format(session.get_current_user()))\n",
    "print('Role      : {}'.format(session.get_current_role()))\n",
    "print('Database  : {}'.format(session.get_current_database()))\n",
    "print('Schema    : {}'.format(session.get_current_schema()))\n",
    "print('Warehouse : {}'.format(session.get_current_warehouse()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9f22ce6-da50-4d76-98e1-3ff6246ed220",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_read_diamonds",
    "resultHeight": 153
   },
   "source": [
    "### Use the Snowpark DataFrame Reader to read in data from the externally staged `diamonds` CSV file \n",
    "\n",
    "In setup.sql, we staged the `diamonds.csv` file from an external s3 bucket. Now, we can read it in.\n",
    "\n",
    "For more information on loading data, see documentation on [snowflake.snowpark.DataFrameReader](https://docs.snowflake.com/ko/developer-guide/snowpark/reference/python/api/snowflake.snowpark.DataFrameReader.html).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6080dc92-595e-48b6-b4fc-667e2346bed9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "read_diamonds",
    "resultHeight": 439
   },
   "outputs": [],
   "source": [
    "# Create a Snowpark DataFrame that is configured to load data from the CSV file\n",
    "# We can now infer schema from CSV files.\n",
    "diamonds_df = session.read.options({\"field_delimiter\": \",\",\n",
    "                                    \"field_optionally_enclosed_by\": '\"',\n",
    "                                    \"infer_schema\": True,\n",
    "                                    \"parse_header\": True}).csv(\"@DIAMONDS_ASSETS\")\n",
    "\n",
    "diamonds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04f72db-d8b4-4f25-aaf0-928475009895",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "describe_diamonds",
    "resultHeight": 252
   },
   "outputs": [],
   "source": [
    "# Look at descriptive stats on the DataFrame\n",
    "diamonds_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8205f888-1a4a-4033-b4f4-ddbb1bc30e67",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "diamonds_cols",
    "resultHeight": 360
   },
   "outputs": [],
   "source": [
    "diamonds_df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8babcdbe-083d-4e4e-b7ec-ac86982e775d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_clean_data",
    "resultHeight": 113
   },
   "source": [
    "### Data cleaning\n",
    "\n",
    "First, let's force headers to uppercase using Snowpark DataFrame operations for standardization when columns are later written to a Snowflake table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd0ce8f-0f7d-4cc9-ad07-f59d5c8e67a2",
   "metadata": {
    "language": "python",
    "name": "uppercase_headers",
    "resultHeight": 439
   },
   "outputs": [],
   "source": [
    "# Force headers to uppercase\n",
    "for colname in diamonds_df.columns:\n",
    "    if colname == '\"table\"':\n",
    "       new_colname = \"TABLE_PCT\"\n",
    "    else:\n",
    "        new_colname = str.upper(colname)\n",
    "    diamonds_df = diamonds_df.with_column_renamed(colname, new_colname)\n",
    "\n",
    "diamonds_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93794112-57e4-442e-b598-8c87aa31594e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_std_formatting",
    "resultHeight": 134
   },
   "source": [
    "Next, we standardize the category formatting for `CUT` using Snowpark DataFrame operations.\n",
    "\n",
    "This way, when we write to a Snowflake table, there will be no inconsistencies in how the Snowpark DataFrame will read in the category names. Secondly, the feature transformations on categoricals will be easier to encode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03328f0-f85a-4296-bfce-923a44aeadf7",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "std_formatting"
   },
   "outputs": [],
   "source": [
    "def fix_values(columnn):\n",
    "    return F.upper(F.regexp_replace(F.col(columnn), '[^a-zA-Z0-9]+', '_'))\n",
    "\n",
    "for col in [\"CUT\"]:\n",
    "    diamonds_df = diamonds_df.with_column(col, fix_values(col))\n",
    "\n",
    "diamonds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654c8a95-271b-440b-a579-8f42d8f6f135",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_schema_check",
    "resultHeight": 41
   },
   "source": [
    "Check the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989ec169-24e9-477f-8170-a5c332249269",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "check_schema"
   },
   "outputs": [],
   "source": [
    "list(diamonds_df.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09273ca-c6a9-461f-8697-c703b62b986f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_cast_decimal"
   },
   "source": [
    "Finally, let's cast the decimal types to DoubleType() since DecimalType() isn't support by Snowpark ML at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988261a8-7619-48c8-8241-6aa490e6d2d4",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cast_decimal"
   },
   "outputs": [],
   "source": [
    "for colname in [\"CARAT\", \"X\", \"Y\", \"Z\", \"DEPTH\", \"TABLE_PCT\"]:\n",
    "    diamonds_df = diamonds_df.with_column(colname, diamonds_df[colname].cast(DoubleType()))\n",
    "\n",
    "diamonds_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54aea408-5f24-4fc1-9add-de83caed2b05",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_write_data"
   },
   "source": [
    "### Write cleaned data to a Snowflake table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d00d0c5-157e-461b-b3e7-6577e081935d",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "write_data"
   },
   "outputs": [],
   "source": [
    "diamonds_df.write.mode('overwrite').save_as_table('diamonds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a0fb14-608b-49c0-87e8-721c71cb5688",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_next_nb"
   },
   "source": [
    "In the next notebook, we will perform data transformations with the Snowpark ML Preprocessing API for feature engineering. "
   ]
  }
 ]
}